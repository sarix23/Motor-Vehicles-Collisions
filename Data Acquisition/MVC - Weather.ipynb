{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Open Meteo API - Hourly Weather Data for NYC's Zip Codes in 2018**"]},{"cell_type":"markdown","metadata":{},"source":["## Import Packages"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import openmeteo_requests\n","import requests_cache\n","from retry_requests import retry"]},{"cell_type":"markdown","metadata":{},"source":["## NYC Zip Codes\n","Starting from the dataset (https://data.cityofnewyork.us/City-Government/Broadband-Adoption-and-Infrastructure-by-Zip-Code/qz5f-yx82) obtained from the same website from which the accident data were extracted, all zip codes associated with the city of New York were retrieved."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Read the dataset containing\n","nyc_zipcodes_data = pd.read_csv('Broadband_Adoption_and_Infrastructure_by_Zip_Code_20240209.csv')\n","\n","# Filter the necessary data and remove duplicates\n","zip_codes = list(nyc_zipcodes_data['Zip Code'].dropna().drop_duplicates())\n","zip_codes = [int(x) for x in zip_codes]"]},{"cell_type":"markdown","metadata":{},"source":["## Retrieve Latitude and Longitude for each Zip Code\n","Using **web scraping** from the website https://www.zipdatamaps.com/{zip_code}, it was possible to extract the latitude and longitude of each zip code in New York City previously obtained."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43571,"status":"ok","timestamp":1707217049421,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"mHq-TSdI01Es","outputId":"b0e25148-241a-4fd0-a33f-314b81d7c9e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Coordinates not found for the zip code 83\n","Coordinates not found for the zip code 10047\n","Coordinates not found for the zip code 10048\n","Coordinates not found for the zip code 10096\n","Coordinates not found for the zip code 10097\n","Coordinates not found for the zip code 10196\n","    Zip Code   Latitude  Longitude\n","0      10001  40.750244 -73.997017\n","1      10002  40.713882 -73.985924\n","2      10003  40.731991 -73.988869\n","3      10004  40.694939 -74.016922\n","4      10005  40.706150 -74.008568\n","..       ...        ...        ...\n","237    11691  40.600616 -73.762558\n","238    11692  40.595150 -73.796173\n","239    11693  40.611607 -73.815712\n","240    11694  40.575130 -73.851662\n","241    11697  40.557446 -73.913467\n","\n","[242 rows x 3 columns]\n"]}],"source":["# Create an empty dataframe to store the results\n","result_df = pd.DataFrame(columns=[\"Zip Code\", \"Latitude\", \"Longitude\"])\n","\n","# Loop through the zip codes in the list\n","for zip_code in zip_codes:\n","    url = f\"https://www.zipdatamaps.com/{zip_code}\"\n","    page = requests.get(url)\n","    soup = BeautifulSoup(page.content, \"html.parser\")\n","    coordinates = soup.find_all('td')\n","\n","    # Find the index of the \"Coordinates\" element in the list\n","    coordinates_index = None\n","    for i, td in enumerate(coordinates):\n","        if \"Coordinates\" in str(td):\n","            coordinates_index = i\n","            break\n","\n","    # If the index of \"Coordinates\" is found, take the next element and split it\n","    if coordinates_index is not None and coordinates_index + 1 < len(coordinates):\n","        coordinates_text = coordinates[coordinates_index + 1].text.strip()\n","\n","        # Split the coordinates into latitude and longitude\n","        lat, lon = map(float, coordinates_text.split(','))\n","\n","        # Create a temporary dataframe for the current element\n","        temp_df = pd.DataFrame({\"Zip Code\": [zip_code], \"Latitude\": [lat], \"Longitude\": [lon]})\n","\n","        # Concatenate the temporary dataframe with the main dataframe\n","        result_df = pd.concat([result_df, temp_df], ignore_index=True)\n","    else:\n","        print(f\"Coordinates not found for the zip code {zip_code}\")\n","\n","# Print the final dataframe\n","print(result_df)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Open Meteo - API Requests"]},{"cell_type":"markdown","metadata":{},"source":["The **Open Meteo API** (https://open-meteo.com/en/docs/historical-weather-api/) is an open API that, by taking as input the geographic coordinates of a location and a time interval, is capable of returning hourly weather conditions of choice for the specified location and period. \n","\n","However, it has some **limitations** on a per-minute, hourly, and daily basis. For this reason, when extracting weather information (specifically, temperature at 2 meters above ground, relative humidity at 2 meters above ground, rainfall level, snow level, and percentage of cloud cover) for **242 different locations**, it was necessary to divide the list of zip codes into two parts, making two separate requests spaced *90 seconds apart* to avoid exceeding the per-minute limit."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1707218142174,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"CrPLxZAW01Ex"},"outputs":[],"source":["latitudes = list(result_df['Latitude'])\n","longitudes = list(result_df['Longitude'])\n","\n","# Split coordinates into two different lists\n","split_point = (len(latitudes)+1)//2\n","subsets_lat = [latitudes[:split_point], latitudes[split_point:]]\n","subsets_long = [longitudes[:split_point], longitudes[split_point:]]"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":419,"status":"ok","timestamp":1707218167154,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"vxSEXvlAaDDA"},"outputs":[],"source":["# Setup the Open-Meteo API client with cache and retry on error\n","cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n","retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n","openmeteo = openmeteo_requests.Client(session = retry_session)\n","\n","# Initalize the url\n","url = \"https://archive-api.open-meteo.com/v1/archive\"\n","\n","# Set the time interval\n","start_date = '2017-12-31'\n","stop_date = '2018-12-31'"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":184999,"status":"ok","timestamp":1707218556892,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"gZjEjz9A01Ey"},"outputs":[],"source":["import time\n","responses = [0]*len(subsets_lat)\n","\n","# Repeat the request for the two lists of coordinates\n","for i in range(len(subsets_lat)):\n","  # Define parameters to make the API request\n","  params = {\n","      \"latitude\": subsets_lat[i],\n","      \"longitude\": subsets_long[i],\n","      \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"rain\", \"snowfall\", \"cloud_cover\"],\n","      \"timezone\": \"auto\",\n","      \"start_date\": start_date,\n","      \"end_date\": stop_date\n","  }\n","\n","  # Make the API request\n","  response = openmeteo.weather_api(url, params=params)\n","  responses[i] = response\n","  time.sleep(90)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1707218704380,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"bpdfUbZIKQKu","outputId":"867f9554-dfc7-44f7-d884-aba0e3b20884"},"outputs":[{"data":{"text/plain":["242"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Concat the two lists of responses in a unique list\n","resp = []\n","resp.extend(responses[0])\n","resp.extend(responses[1])"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4496,"status":"ok","timestamp":1707218855228,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"Y6QNnQsH01Ez"},"outputs":[],"source":["# Create an empty dataframe in which collect hourly weather data of 2018, for each location\n","weather_dataset = pd.DataFrame()\n","\n","# Process location's data\n","for location in range(len(resp)):\n","\tresponse = resp[location]\n","\n","\t# Process hourly data. The order of variables needs to be the same as requested.\n","\thourly = response.Hourly()\n","\thourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n","\thourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n","\thourly_rain = hourly.Variables(2).ValuesAsNumpy()\n","\thourly_snowfall = hourly.Variables(3).ValuesAsNumpy()\n","\thourly_cloud_cover = hourly.Variables(4).ValuesAsNumpy()\n","\n","\thourly_data = {\"date\": pd.date_range(\n","\t\tstart = pd.to_datetime(hourly.Time(), unit = \"s\"),\n","\t\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\"),\n","\t\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n","\t\tinclusive = \"left\"\n","\t)}\n","\n","\t# Save data into the dataframe\n","\thourly_data[\"temperature_2m\"] = hourly_temperature_2m\n","\thourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n","\thourly_data[\"rain\"] = hourly_rain\n","\thourly_data[\"snowfall\"] = hourly_snowfall\n","\thourly_data[\"cloud_cover\"] = hourly_cloud_cover\n","\thourly_data['zip_code'] = list(result_df['Zip Code'])[location]\n","\n","\thourly_dataframe = pd.DataFrame(data = hourly_data).sort_values(by='date')\n","\tweather_dataset = pd.concat([weather_dataset, hourly_dataframe])"]},{"cell_type":"markdown","metadata":{},"source":["When a time interval is inputted to the API, it returns results from 05:00:00 of the start date (which is why the start date was set to 2018-12-31) to 04:00:00 of the day following the end date. Due to the large amount of downloaded data, to reduce the dataset size, non-relevant time intervals were removed."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":361,"status":"ok","timestamp":1707218862791,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"VyV6H4NU01Ez"},"outputs":[],"source":["weather_dataset = weather_dataset[(weather_dataset['date']>= '2018-01-01 00:00:00') & (weather_dataset['date']<= '2018-12-31 23:59:59')]"]},{"cell_type":"markdown","metadata":{},"source":["## Export Data\n","Export data into a csv file."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":20336,"status":"ok","timestamp":1707218894783,"user":{"displayName":"Sara Nava","userId":"03606279834411750883"},"user_tz":-60},"id":"_fmR9Zeq01E0"},"outputs":[],"source":["weather_dataset.to_csv('Weather.csv', index = False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.undefined"}},"nbformat":4,"nbformat_minor":0}
